\documentclass{article}

% --- packages ---

\usepackage[utf8]{inputenc}
\usepackage[english, ngerman]{babel}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{amsthm}
\usepackage{mathdots}
\usepackage[linesnumbered,ruled,noline]{algorithm2e}
\usepackage{tikz, pgfplots}
\usepackage{amssymb}
\usepackage{graphicx}
%\usepackage{subcaption}
\usepackage{multirow}
\usepackage{pgf}
\usepackage{caption}
\usepackage{listings, lstautogobble}
\usepackage{delarray}
\usepackage{bigints}
\usepackage{float}
\usepackage{layouts}
\usepackage{subfigure}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{MnSymbol}
\usepackage{wasysym}
\usepackage[a]{esvect} %für bessere Vektorpfeile
\usepackage{setspace}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{array}
\usepackage{acronym}
\usepackage{ifsym}
\usepackage[sorting=nty]{biblatex}
\usepackage[nottoc,numbib]{tocbibind}

% --- Input macros von Ide ---

%\input{macros}

% --- Seitenränder / Einrücktiefe ---

\geometry{left=2.5cm, right=2.5cm, top=2cm, bottom=2cm}
\parindent0cm

% --- new commands ---

\newcommand{\RM}[1]{\MakeUppercase{\romannumeral #1{}}} %für römische Zahlen #1{}}} %für römische Zahlen
\newcommand{\xvec}{\underline{x}}
\newcommand{\bvec}{\underline{b}}
\newcommand{\uvec}{\underline{u}}
\newcommand{\vvec}{\underline{v}}
\newcommand{\wvec}{\underline{w}}
\newcommand{\pvec}{\underline{p}}
\newcommand{\zvec}{\underline{z}}
\newcommand{\xstern}{\underline{x}^*}
\newcommand{\Rmn}{\mathbb{R}^{m \times n}}
\newcommand{\Elasticsearch}{\textit{Elasticsearch}\;}
\newcommand{\Rnn}{\mathbb{R}^{n \times n}}
\newcommand{\rang}{\text{rang}}
\def\code#1{\texttt{#1}}
\newcommand{\dom}{\textrm{dom}}
\newcommand{\lojoin}{{\tiny \textifsym{d|><|}}}
\newcommand{\rojoin}{{\tiny \textifsym{|><|d}}}
\newcommand{\fojoin}{{\tiny \textifsym{d|><|d}}}
\newcommand{\m}{\cdot} %Malzeichen einfacher
\newcommand{\entspricht}{$\mathop{\hat{=}}$} %Entspricht-Zeichen
\renewcommand{\labelnamepunct}{\addcolon\space}

% -- Definitionen für Theoremumgebungen ---

\newtheoremstyle{newline}% name
{}% Space above
{\baselineskip}% Space below
{\normalfont}% Body font
{}% Indent amount
{\bfseries}% Theorem head font
{:}% Punctuation after theorem head
{\newline}% Space after theorem head
{\thmname{#1}\thmnumber{ #2}\thmnote{ (#3)}}% Theorem head spec (can be left empty, meaning ‘normal’ )

\theoremstyle{newline}
\newtheorem{definition}{Definition}[section]
\newtheorem{satz}[definition]{Satz}
\newtheorem{bemerkung}[definition]{Bemerkung}
\newtheorem{folgerung}[definition]{Folgerung}

\renewenvironment{proof}[1][\proofname:]{%
\minisec{#1}
\pushQED{\qed}%
\itshape
}{%
\popQED
\par
\medskip
}

% --- listing_settings ---
\lstset{
	language=Python,
	frame=single, % Rahmen um den Code
	numbers=left, % Zeilennummern auf der linken Seite
	numberstyle=\small, % Stil der Zeilennummern
	xleftmargin=2em, % Abstand zum linken Rand
	keywordstyle=\color{blue}, % Kewyord Style
	tabsize=4,
	breaklines=true,
}

% --- options for algorithm
\SetKwComment{Comment}{/* }{ */}


% --- Define colors ---

\definecolor{red}{rgb}{0.6,0,0} % for strings
\definecolor{blue}{rgb}{0,0,0.6}
\definecolor{green}{rgb}{0,0.8,0}
\definecolor{cyan}{rgb}{0.0,0.6,0.6}
\newcommand{\greencomment}[1]{\textcolor{green}{#1}}


\lstset{autogobble=true}

% --- Literaturverzeichnis ---
\bibliography{nserl.bib}


% --- Beginn des eigentlichen Dokuments ---


\begin{document}

% -- Layout Kopfzeile ---

\pagestyle{fancy}
\lhead{\small{\slshape}}
\chead{}	
\rhead{\mdseries \leftmark}


\begin{titlepage}

%\includegraphics[width=0.20\textwidth]{Bilder/fh_logo.png}\hfill\includegraphics[width=0.25\textwidth]{Bilder/diamant-software-logo-teams.png}

% --- include correct images here

\centering

{\bfseries HSBI Bielefeld \par}
\vspace{0.25cm}
{\bfseries University of Applied Sciences \par}
\vspace{0.25cm}
{\bfseries Fachbereich Ingenieurwissenschaften und Mathematik \par}
\vspace{0.25cm}
{\bfseries Studiengang Optimierung und Simulation \par}
\vspace{1.5cm}
{\huge\bfseries Lösen von nichtlinearen Gleichungssystemen mit einem Reinforcement-Learning-Agent \par}
\vspace{2.5cm}
{\Large\bfseries Bericht \par}
{\vspace{8.5cm}}

\flushleft
\begin{tabular}{ll}
	Vorgelegt von: & Nicolas Schneider \vspace{0.25cm}\\
	Matrikelnummer: & 1208960 \vspace{0.25cm} \\
	Studiengang: & Optimierung und Simulation \vspace{0.5cm} \\
	Abgabedatum: & 07.04.2024 \vspace{0.5cm} \\
	Betreuer: & Prof. Dr. rer. nat. Bernhard Bachmann
\end{tabular}
\vfill
\end{titlepage}


\thispagestyle{empty}
\newpage



\begin{onehalfspace}

\thispagestyle{empty}
\selectlanguage{english}
\begin{abstract}
	Nichtlineare Gleichungssysteme (NGS) spielen eine zentrale Rolle in vielen Bereichen der Wissenschaft und Technik, da sie zur Modellierung und Lösung komplexer Probleme in Physik, Chemie, Ingenieurwesen und anderen Disziplinen eingesetzt werden. Trotz ihrer Bedeutung stellt die Lösung von NGS aufgrund ihrer inhärenten Nichtlinearität und des Fehlens geschlossener analytischer Lösungen eine große Herausforderung dar. Traditionelle numerische Verfahren wie das Newton-Raphson-Verfahren oder Optimierungsansätze stoßen oft an ihre Grenzen, insbesondere bei hochdimensionalen Problemen, chaotischem Verhalten oder starker Abhängigkeit von Anfangsbedingungen.
	
	In jüngster Zeit hat der Bereich des Reinforcement learnings (RL) zunehmend an Bedeutung gewonnen und vielversprechende Ergebnisse bei der Lösung komplexer Probleme geliefert. RL-Agenten lernen durch Interaktion mit einer Umgebung und Belohnungssignale, optimale Strategien zu entwickeln, ohne explizite Programmierung. Dieser Ansatz hat sich in verschiedenen Anwendungsfeldern wie Robotik, Spielen und Optimierungsproblemen als erfolgreich erwiesen.
	\\
	
	In dieser Arbeit wird der Ansatz des RL verwendet, um Lösungen für nichtlineare Gleichungssysteme zu finden. Ein RL-Agent wird in einer maßgeschneiderten Umgebung trainiert, die die Struktur des gegebenen nichtlinearen Gleichungssystems widerspiegelt. Durch die Formulierung von Belohnungen für Aktionen, die den Agenten näher an eine Lösung führen, wird dieser befähigt, iterative Strategien zur effizienten Lösungsfindung zu erlernen.
	
	Die vorliegende Arbeit präsentiert einen initiierenden Ansatz und analysiert seine Leistung hinsichtlich Schnelligkeit und Genauigkeit bei der Lösungsfindung von nichtlinearen Gleichungssystemen. Dabei wird eine erste Untersuchung durchgeführt, um die Effektivität dieses Ansatzes im Vergleich zu etablierten Methoden wie dem Newton-Raphson-Verfahren zu bewerten. Besonderes Augenmerk wird auf die Identifizierung und Analyse der limitierenden Faktoren dieses Ansatzes gelegt, um potenzielle Schwächen aufzudecken und zu verstehen, inwiefern dieser Ansatz für praktische Anwendungen geeignet ist.
	
	% Hier noch Ergebnisse kurz darstellen?
	
\end{abstract}
\selectlanguage{ngerman}
\newpage


\tableofcontents	% Inhaltsverzeichnis		
\thispagestyle{empty}

\newpage		

\section{Grundlagen nichtlinearer Gleichungssysteme}

Ein nichtlineares Gleichungssystem kann als Nullstellenproblem wie folgt formuliert werden:

\begin{definition}[Nichtlineares Gleichungssystem]
	Es sei $B \subset \mathbb{R}^n$ und $\mathbf{g}: B \rightarrow \mathbb{R}^n$. Gesucht sind Lösungen von
	
	\begin{center}
		$\mathbf{f}(\mathbf{x}) = \mathbf{0}.$
	\end{center}
	
	$\mathbf{f}(\mathbf{x}) = \mathbf{0}$ ist ein System von $n$ nichtlinearen Gleichungen für $n$ Unbekannte $x_1, \dots, x_n$.
	\\
	
	\begin{center}
		\boxed{
			\begin{array}{ccc}
				f_1(x_1, \dots, x_n) & = &0\\
				f_2(x_1, \dots, x_n) & = & 0\\
				\vdots &&\\
				f_n(x_1, \dots, x_n) & = & 0
			\end{array}
		}
	\end{center}
	\cite{Merzigera}
\end{definition}
\bigskip

Nichtlineare Gleichungssysteme treten in vielen Bereichen der Wissenschaft und Technik auf, wie beispielsweise in der Physik, Chemie, Biologie, Wirtschaftswissenschaften und Ingenieurwissenschaften. Sie dienen zur Modellierung und Analyse komplexer Systeme, deren Verhalten durch nichtlineare Beziehungen zwischen den Variablen beschrieben wird.

Die Lösung solcher Systeme ist jedoch oft eine große Herausforderung, da nichtlineare Gleichungen im Allgemeinen keine geschlossenen analytischen Lösungen besitzen und zu den $NP$-schweren Probleme zählen. Stattdessen müssen numerische Verfahren eingesetzt werden, um approximative Lösungen zu finden. Einige gängige Methoden zur Lösung nichtlinearer Gleichungssysteme sind:

\begin{enumerate}
	\item \textbf{Iterative Verfahren}: Hierzu zählen Methoden wie das Newton-Raphson-Verfahren, das Quasi-Newton-Verfahren und das Broyden-Verfahren. Diese Verfahren starten mit einer Anfangsschätzung und verbessern diese iterativ, bis eine ausreichend genaue Lösung gefunden ist.
	
	\item \textbf{Globale Optimierungsverfahren}: Wenn das Gleichungssystem als Optimierungsproblem formuliert werden kann, können globale Optimierungsverfahren wie die Branch-and-Bound-Methode oder evolutionäre Algorithmen zur Lösung eingesetzt werden.
\end{enumerate}

Die Schwierigkeit, nichtlineare Gleichungssysteme zu lösen, besteht oft darin, geeignete Anfangsschätzungen für die Iterationsverfahren zu finden und die Konvergenz der Verfahren sicherzustellen. Viele Systeme weisen mehrere Lösungen auf, von denen einige instabil oder nicht physikalisch sinnvoll sein können. Darüber hinaus können nichtlineare Systeme eine komplexe Struktur mit mehreren lokalen Extrema aufweisen, was die globale Konvergenz erschwert. (vgl. \cite{Mainzer1999})


\section{Grundlagen Reinforcement Learning}

Reinforcement learning (RL) ist ein Teilgebiet des Maschinellen Lernens, bei dem ein Agent lernt, durch Interaktion mit einer Umgebung eine bestimmte Aufgabe oder ein Ziel zu erreichen. Im Gegensatz zu überwachtem Lernen, bei dem ein Modell anhand von Trainingsbeispielen mit bekannten Eingabe-Ausgabe-Paaren gelernt wird, erhält der Agent beim Reinforcement learning nur eine skalare Bewertung (Reward) für seine Aktionen. Durch Ausprobieren und Lernen aus den erhaltenen Rewards versucht der Agent, eine Strategie (Policy) zu finden, die die kumulative Belohnung über die Zeit maximiert.
\\

\textbf{Markov-Entscheidungsprozess:}
\smallskip

Reinforcement Learning Probleme werden häufig als Markov-Entscheidungsprozesse (Markov Decision Processes, MDPs) formuliert. Ein MDP besteht aus:

\begin{itemize}
	\item Einem Zustandsraum $\mathcal{S}$, der alle möglichen Zustände der Umgebung enthält.
	\item Einem Aktionsraum $\mathcal{A}$, der alle möglichen Aktionen des Agenten definiert.
	\item Einer Übergangswahrscheinlichkeitsfunktion $\mathcal{P}(s, a, s')$, die die Wahrscheinlichkeit angibt, dass der Agent beim Ausführen der Aktion $a$ im Zustand $s$ in den Zustand $s'$ übergeht.
	\item Einer Belohnungsfunktion $\mathcal{R}(s, a, s')$, die die Belohnung definiert, die der Agent erhält, wenn er aus dem Zustand $s$ durch Ausführen der Aktion $a$ in den Zustand $s'$ übergeht.
\end{itemize}

Das Ziel des Agenten ist es, eine Policy $\pi: \mathcal{S} \rightarrow \mathcal{A}$ zu finden, die die erwartete kumulative Belohnung über die Zeit maximiert.

Abbildung \ref{fig:rl_grundlagen} stellt den Lernprozess eines Agenten in einer Umgebung vereinfacht dar.

\begin{figure}[h]
	\centering
	\includegraphics[width=0.5\textwidth]{Bilder/rl_grundlagen.png}
	\caption{Vereinfachter Lernprozess eines Agenten in einer Umgebung\protect\footnotemark}
	\label{fig:rl_grundlagen}
\end{figure}
\footnotetext{\url{https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292}}


\textbf{Value Functions und Bellman-Gleichungen:}
\smallskip

Eine zentrale Rolle beim Reinforcement Learning spielen die Value Functions, die den erwarteten kumulativen Reward für einen gegebenen Zustand oder eine Zustand-Aktions-Paar angeben. Es gibt zwei wichtige Value Functions:

\begin{itemize}
	\item Die State-Value Function $V^\pi(s)$ gibt den erwarteten kumulativen Reward an, wenn der Agent im Zustand $s$ startet und der Policy $\pi$ folgt.
	\item Die Action-Value Function $Q^\pi(s, a)$ gibt den erwarteten kumulativen Reward an, wenn der Agent im Zustand $s$ startet, die Aktion $a$ ausführt und danach der Policy $\pi$ folgt.
\end{itemize}

Die Value Functions erfüllen die Bellman-Gleichungen, die eine rekursive Beziehung zwischen den Value Functions benachbarter Zustände herstellen. Für die State-Value Function lautet die Bellman-Gleichung:

\begin{equation}
	V^\pi(s) = \mathbb{E}_\pi \left[ R(s, a, s') + \gamma V^\pi(s') \right]
\end{equation}

Und für die Action-Value Function:

\begin{equation}
	Q^\pi(s, a) = \mathbb{E}_\pi \left[ R(s, a, s') + \gamma \sum_{s'} \mathcal{P}(s, a, s') V^\pi(s') \right]
\end{equation}

Hier ist $\gamma \in [0, 1]$ der Diskontierungsfaktor, der bestimmt, wie viel Gewicht zukünftigen Rewards beigemessen wird. Ein Wert von $\gamma$ nahe 0 bedeutet, dass der Agent nur die unmittelbaren Rewards optimiert, während ein Wert nahe 1 längerfristige Belohnungen stärker gewichtet. (vgl. \cite{Lia2022})
\\

Dieses Kapitel bietet eine grundlegende Einführung in das Reinforcement Learning (RL), einem Teilgebiet des Maschinellen Lernens. RL ermöglicht es einem Agenten, durch Interaktion mit seiner Umgebung eine bestimmte Aufgabe zu erlernen, indem er Belohnungen für seine Aktionen maximiert, ohne explizite Trainingsdaten zu erhalten.

Der Markov-Entscheidungsprozess (MDP) bildet den Kern eines RL-Problems, definiert durch einen Zustandsraum, einen Aktionsraum, eine Übergangsfunktion und eine Belohnungsfunktion. Dabei spielen Value Functions und Bellman-Gleichungen eine entscheidende Rolle, um den erwarteten kumulativen Reward zu bewerten.

Es ist wichtig zu betonen, dass die Umsetzung eines RL-Agenten stark vom Anwendungsfall und den verfügbaren Daten abhängt, was zu einer großen Variabilität in der Implementierung führt. Dieses grundlegende Verständnis dient als Ausgangspunkt für die weiterführende Diskussion darüber, wie RL bei der Lösung nichtlinearer Gleichungssysteme eingesetzt werden kann. 

\section{Reinforcement Learning für die Lösung nichtlinearer Gleichungssysteme}

Bevor ein RL-Agent nichtlineare Gleichungssysteme lösen kann, sollten zunächst einige wichtige Fragen beantwortet werden, die für die Qualität der Lösung und die Lösungsfindung von Relevanz sind:

\begin{enumerate}
	\item \textbf{Umgebung:} Was ist die Umgebung, in dem der Agent agiert?
	\item \textbf{Zustand:} Was sind zulässige und sinnvolle Zustände?
	\item \textbf{Aktionen:} Was sind zulässige und sinnvolle Aktionen, die der Agent wählen kann?
	\item \textbf{Belohnung:} Wie sieht eine zielführende Belohnungsfunktion aus?
\end{enumerate}

Diese Projektarbeit fokussiert sich auf einen ersten Ansatz, der hinsichtlich seiner Effektivität untersucht wird.

Für die oben genannten Fragen wurden folgende erste Lösungsansätze entwickelt:
\\

\underline{Umgebung:}
\smallskip

Im Rahmen dieses Projekts wird die Umgebung als der Raum definiert, in dem die Gleichungen existieren. Ein wesentliches Kennzeichen ist, dass die Umgebung als statisch angesehen wird, solange die Gleichungen und dementsprechend auch die Lösungen während des Lösungsvorgangs unverändert bleiben.
\\

\underline{Zustand:}
\smallskip

Der Zustand wird aufgefasst, als die gewählte Aktion.
\\

\underline{Aktion:}
\smallskip

Abhängig von der Dimensionalität des Problems kann der Agent aus einem kontinuierlichen Raum von Punkten wählen. Der Agent wählt eine Aktion $a \in \mathcal{A}$, wobei $\mathcal{A} \subseteq \mathbb{R}^{n}$ und $n$ die Dimensionalität des Problems widerspiegelt.
\\
 
\underline{Belohnungsfunktion:}
\smallskip

Die Formulierung der Belohnungsfunktion konzentriert sich darauf, die Lösung(en) des Gleichungssystems zu finden, sofern sie denn existieren. In dieser Arbeit wird das Residuum verwendet, welches den Fehler zur eigentlichen Lösung darstellt. Das Residuum $R$ an der Optimallösung $\mathbf{x}^{\ast}$ ist $\mathbf{0}$, sodass das Ziel dabei ist diesen Fehler zu minimieren. 

\begin{definition}[Residuum]
	Gegeben seien $n$ nichtlineare Gleichungen ($f_1(\mathbf{x}), f_2(\mathbf{x}), \dots, f_n(\mathbf{x})$) mit $\mathbb{R}^n \rightarrow \mathbb{R}^n$ gegeben. Es sei vorausgesetzt, dass mindestens eine Lösung $\mathbf{x}^{\ast}$ existiert, die das Gleichungssystem löst. Dann ist das Residuum $R \in \mathbb{R}^{\geq 0}$ an der Stelle $\mathbf{x}$ mit $\mathbf{x} \in \mathbb{R}^n$ definiert als
	
	\begin{equation}
		R(\mathbf{x}) = ||\mathbf{f}(\mathbf{x})||_2 = \sqrt{ \sum_{i=1}^{n} f_i(\mathbf{x})^2 }.
	\end{equation}
\end{definition}

Im Zuge der Annäherung an die Lösung und der damit einhergehenden Fehlerreduktion lässt sich die Belohnung als das Inverse des Residuums darstellen.

\begin{definition}[Belohnung]
	\begin{equation}
		Reward = \dfrac{c}{R(\mathbf{x}) + 1} \quad c \in \mathbb{R}
	\end{equation}
\end{definition}

Folglich steigt die Belohnung proportional zur Verringerung des Residuums, was bedeutet, dass eine kontinuierliche Reduktion des Residuums zu einer zunehmend größeren Belohnung führt. Das Hauptziel dabei ist, den Agenten zu ermutigen, sich der Lösung kontinuierlich anzunähern. Der Parameter $c$ spiegelt dabei die maximal erreichbare Belohnung wieder. Für die Umsetzung innerhalb dieses Projektes wurde $c=1$ angenommen.
\\

Dieses Kapitel soll die grundsätzlichen Fragen beantworten, die bei der Kombination von RL und nichtlinearen Gleichungssystemen auftreten können. Diese müssen vor der tatsächlichen Umsetzung mit Bedacht beantwortet werden, sodass eine zielführende Lösungsfindung garantiert ist.

Besonderes Augenmerk gilt dabei der Umgebung, dem Zustands- sowie Aktionsraum und der Belohnungsfunktion des Agenten.

Da die Grundlagen für nichtlineare Gleichungssysteme, sowie für RL gelegt worden sind, wird im nachfolgenden Kapitel der Frage nachgegangen, wie diese Problemstellung genau umgesetzt werden kann.

\section{Umsetzung}

Bevor mit der Implementierung der Problemstellung, der Umgebung und des Agenten begonnen wird, ist es von entscheidender Bedeutung, die Auswahl der richtigen Programmiersprache und Bibliothek zu prüfen. Diese Entscheidung bildet das Fundament für das gesamte Projekt und kann einen erheblichen Einfluss auf die Effizienz, Skalierbarkeit und den Erfolg der Umsetzung haben.
\medskip

Für dieses Projekt wurde die Programmiersprache \textit{Python} als Programmiersprache gewählt, zusammen mit der Reinforcement-Learning-Bibliothek \textit{stable-baselines3}. Diese Entscheidung basiert darauf, dass \textit{stable-baselines3} eine breite Palette von wichtigen RL-Algorithmen implementiert und gleichzeitig eine benutzerfreundliche Integration mit \textit{Python} bietet. Außerdem bietet diese Bibliothek eine ausgezeichnete Kompatibilität mit der Programm-Bibliothek \textit{gymnasium} von OpenAI, die essenziell für Erstellung von RL-Umgebungen ist. Die Kombination aus \textit{Python}, \textit{stable-baselines3} und \textit{gymnasium} ermöglicht eine effiziente Entwicklung und Umsetzung von Reinforcement-Learning-Algorithmen für die gegebenen Anforderungen des Projekts (vgl. \cite{Raffin2021}, vgl. \cite{Towers2023}). Der Programm-Code kann im folgenden Repository eingesehen werden: \url{https://github.com/Nicolas2912/OuS_Projekt}.
\\

Für die Umsetzung des Agenten bzw. der Umgebung sind folgende Methoden für die Problemstellung relevant:

\begin{itemize}
	\item \textbf{\code{get\_distance(x)}}: Berechnet das Residuum für einen Vektor $\mathbf{x} \in \mathbb{R}^{n}$.
	\item \textbf{\code{reward(residuum, *args)}}: Berechnet den Reward basierend auf dem Residuum und weiteren zusätzlichen Parametern.
	\item \textbf{\code{step(action)}}: Führt einen Schritt während des Lernens in der Umgebung aus und aktualisiert dabei die Belohnung auf Grundlage verschiedener, vorher festgelegter Regeln.
	\item \textbf{\code{reset()}}: Setzt die Umgebung auf einen zufälligen Zustand zurück, sobald eine bestimmte Bedingung erfüllt ist.
\end{itemize}
\bigskip

Auch der RL-Algorithmus ist entscheidend für die Lösungsfindung, da er die Strategie bestimmt, nach der der Agent in der Umgebung handelt und lernt. Im Verlauf des Projekts wurden verschiedene Algorithmen aus der \textit{stable-baselines3}-Bibliothek getestet. Unter diesen hat sich der \textit{Proximal Policy Algorithmus} (PPO) als besonders geeignet für diese spezifische Problemstellung erwiesen. Für eine detaillierte Beschreibung dieses Algorithmus sei auf \cite{Schulman2017} verwiesen.
\medskip

\underline{Umgebung:}
\smallskip

Neben vielen vorgegebenen Umgebungen, die in der Programm-Bibliothek \textit{gymnasium} implementiert sind wie z.B. \textit{Cart Pole}, \textit{Acrobot} oder \textit{Pendulum} ist es auch möglich eine eigene Umgebung für einen speziellen Anwendungsfall zu erstellen. Für die Problemstellung nichtlineare Gleichungssysteme mit einem RL-Agent zu lösen, wird eine eigene Umgebung erstellt. 

Nachfolgend ist eine verkürzte Initialisierung der Umgebung dargestellt.

\begin{lstlisting}[language=Python, caption={Initialisierung der Umgebung}, label={lst:python}]
	class CustomEnv(gym.Env):
		def __init__(self, dimension):
			super(CustomEnv, self).__init__()
			self.dimension = dimension
			self.low_bounds = -5.0 * np.ones(dimension)
			self.high_bounds = 5.0 * np.ones(dimension)
			self.action_space = gym.spaces.Box(low=self.low_bounds, high=self.high_bounds, dtype=np.float64)
			self.observation_space = gym.spaces.Box(low=-np.infty, high=np.infty, shape=(dimension,))
			self.state = np.array([random.uniform(self.x_min, self.x_max), random.uniform(self.y_min, self.y_max)] + [0.0] * dimension)	
\end{lstlisting}

Bei der Initialisierung der Umgebung muss der Bereich der möglichen Aktionen (\code{action\_space}) angegeben werden. Da mögliche Lösungen für ein nichtlineares Gleichungssystem sich im ganzen Raum $\mathbb{R}^n$ befinden können, bietet sich dieser auch für die möglichen Aktionen an. Jedoch ist es nicht möglich den ganzen Raum (von $-\infty$ bis $\infty$) anzugeben, sodass Grenzen gesetzt werden müssen. Diese sind im Code mit den Variablen \code{self.low\_bounds} und \code{self.high\_bounds} beschrieben. Hierbei zeichnet sich auch einer der Nachteile ab. Da bei den allermeisten nichtlinearen Gleichungssystemen von Beginn an nicht abzuschätzen ist, in welchem Teil des Lösungsraumes sich die Lösung(en) befindet/befinden, stellt die Wahl des Aktionsraumes eine Herausforderung dar. Ähnlich wie beim Newton-Raphson-Verfahren, bei dem ein initialer Startpunkt notwendig ist, der die Laufzeit des Verfahrens maßgeblich beeinflussen kann, so kann auch die Größe des Aktionsraumes die Laufzeit der Lösungsfindung maßgeblich beeinflussen.
\bigskip

\underline{Ausführung eines Lernschritts:}
\smallskip

Die Ausführung eines Lernschritts ist entscheidend für die Lösungsfindung und ist genauer in Algorithmus \ref{algo:step} beschrieben.

Zuerst seien die wichtigsten Variablen in Tabelle \ref{tab:Variablendefinition} definiert.

\begin{table}[htbp]
	\centering
	\caption{Variablendefinitionen}
	\label{tab:Variablendefinition}
	\begin{tabular}{>{\itshape}l p{0.6\linewidth}}
		\toprule
		\textnormal{Variable} & \textnormal{Beschreibung} \\
		\midrule
		\text{\code{best\_residuum}} & Das aktuell beste Residuum.\\
		\text{\code{last\_residuum}} & Das Residuum aus dem vorherigen Schritt.\\
		\text{\code{good\_points\_threshold}} & Der Schwellenwert für einen \glqq guten Punkt\grqq{}. \\
		\text{\code{done}} & Ein boolescher Wert, der den Abschluss anzeigt. \\
		\text{\code{consecutive\_no\_improvement}} & Die Anzahl aufeinanderfolgender Iterationen ohne Verbesserung.\\
		\bottomrule
	\end{tabular}
\end{table}
\newpage

\begin{algorithm}
	\caption{Ausschnitt zur Ausführung eines Schrittes}
	\label{algo:step}
	\KwIn{action}
	\small
	\KwOut{reward, done}
	\SetKwData{residuum}{residuum}
	\SetKwData{threshold}{threshold}
	\SetKwData{distancesarray}{distances\_array}
	\SetKwData{improvementrate}{improvement\_rate}
	\SetKwData{return}{return}
	
	\SetAlgoSkip{2em} % Erhöht den vertikalen Abstand zwischen den Zeilen
	
	% Initialization
	\textbf{Initialization:}
	
	\text{\code{residuum}} = \code{get\_distance(action)}, \text{\code{reward}} = \text{\code{residuum}}, \code{done} $=$ \code{False}	
	
	% Check Conditions
	\greencomment{\# Belohung, wenn die derzeitige Aktion besser als die beste Aktion}
	
	 \If{\text{\code{residuum}} $<$ \text{\code{best\_residuum}}}
	{
		\hspace*{1em} $\text{\code{best\_residuum}} = \text{\code{residuum}}$
		
		\hspace*{1em} $\text{\code{reward}} = \text{\code{reward}} + 1$
	}
	
	\greencomment{\# Bestrafung, wenn die derzeitige Aktion schlechter als die beste Aktion ist}
	
	\Else
	{
		\hspace*{1em} $\text{\code{reward}} = \text{\code{reward}} - 0.1$
	}
	
	$\text{\code{improvement}} = \text{\code{residuum}} - \text{\code{last\_residuum}}$
	
	\greencomment{\# Belohnung wenn die gewählte Aktion zu einer Verbesserung führt}
	
	\If{\text{\code{improvement}} $< 0$}
	{
		\hspace*{1em} \text{\code{reward}} $=$ \text{\code{reward}} $+ \;0.1$
	}
	
	\greencomment{\# Bestrafung wenn die gewählte Aktion zu keiner Verbesserung führt}
	
	\Else
	{
		\hspace*{1em} \text{\code{reward}} $=$ \text{\code{reward}} $- \;0.1$
	}
	
	\greencomment{\# Belohnung, wenn die Aktion besser als der dynamische Schwellenwert ist}
	
	\If{\text{\code{residuum}} $<$ \text{\code{good\_points\_thrs}}}
	{
		\hspace*{1em} 	\text{\code{reward}} $=$ \text{\code{reward}} $+ \;0.05$
		
		\hspace*{1em} \text{\code{good\_points\_thrs}} $=$ \text{\code{good\_points\_thrs}} $\cdot\; 0.95$
		
		\hspace*{1em} \text{\code{done}} $=$ \text{\code{True}}
		
		\hspace*{1em}  \text{\code{consecutive\_no\_improvement}} $= 0$
	}
	
	\greencomment{\# Bestrafung, wenn die Aktion schlechter als der dynamische Schwellenwert ist}
	
	\Else
	{
		\hspace*{1em} \text{\code{reward}} $=$ \text{\code{reward}} $-\; 0.01$
		
		\hspace*{1em}  \text{\code{consecutive\_no\_improvement}} $=$ \text{\code{consecutive\_no\_improvement}} $+ 1$
		
		\hspace*{1em} \If{\text{\code{consecutive\_no\_improvement}} $\geq 50$}
		{
			\hspace*{2em} \text{\code{reward}} $=$ \text{\code{reward}} $-\; 0.02$
			
			\hspace*{2em} \text{\code{done}} $=$ \text{\code{True}}
			
		}
	}
	
	return \text{\code{reward}}, \text{\code{done}}
	
\end{algorithm}
\medskip

\textbf{Beschreibung:}
\begin{itemize}
	\item \textbf{Aktualisierung des besten Residuums:}
	\begin{itemize}
		\item Wenn das aktuelle Residuum (\textit{residuum}) geringer ist als das bisher beste aufgezeichnete Residuum (\textit{best\_residuum}):
		\begin{itemize}
			\item Aktualisiere \textit{best\_residuum} auf das aktuelle \textit{residuum}.
			\item Erhöhe die Belohnung um 1,0.
		\end{itemize}
		\item Andernfalls:
		\begin{itemize}
			\item Verringere die Belohnung um 0,1.
		\end{itemize}
	\end{itemize}
	
	\item \textbf{Belohnung für Verbesserung:}
	\begin{itemize}
		\item Wenn die aktuelle Aktion eine Verbesserung gegenüber dem letzten Residuum darstellt (d.h. \textit{improvement} ist negativ):
		\begin{itemize}
			\item Erhöhe die Belohnung um 0,1.
		\end{itemize}
		\item Andernfalls:
		\begin{itemize}
			\item Verringere die Belohnung um 0,1.
		\end{itemize}
	\end{itemize}
	
	\item \textbf{Belohnung und Strafe basierend auf Schwellenwert:}
	\begin{itemize}
		\item Wenn \textit{residuum} unterhalb des Schwellenwertes (\textit{good\_points\_thrs}) liegt:
		\begin{itemize}
			\item Erhöhe die Belohnung um 0,05.
			\item Reduziere den Schwellenwert, indem er mit 0,95 multipliziert wird.
			\item Setze \textit{done} auf Wahr, um ein erfolgreiches Ende zu signalisieren.
			\item Setze den Zähler für aufeinanderfolgende Nicht-Verbesserungen (\textit{consecutive\_no\_improvement}) auf 0 zurück.
		\end{itemize}
		\item Andernfalls:
		\begin{itemize}
			\item Verringere die Belohnung um 0,01.
			\item Erhöhe den Zähler für aufeinanderfolgende Nicht-Verbesserungen (\textit{consecutive\_no\_improvement}) um 1.
			\item Wenn der Zähler für aufeinanderfolgende Nicht-Verbesserungen 50 erreicht:
			\begin{itemize}
				\item Setze \textit{done} auf Wahr, um das Ende aufgrund mangelnder Verbesserung zu signalisieren.
				\item Verringere die Belohnung weiter um 0,02.
			\end{itemize}
		\end{itemize}
	\end{itemize}
\end{itemize}
Folgende nichtlineare Gleichungssysteme wurden für die Auswertung des Verfahrens herangezogen:
\smallskip

\underline{Eindimensionales nichtlineares Gleichungssystem:}

\begin{subequations}\label{nse:1}
	\begin{align}
		x^5 - 3x^4+x^3+0.5x^2 - \sin(2x)&= 0\\
	\end{align}
\end{subequations}

\underline{Zweidimensionale Rosenbrock-Funktion:}

\begin{subequations}\label{nse:rosenbrock}
	\begin{align}
		10(x_2 - x_1^2) &= 0\\
		1 - x_1 &=0
	\end{align}
\end{subequations}

\underline{$n$-dimensionales \textit{Economics Modeling Application}-Problem:} 

\begin{subequations}\label{nse:highdimension}
	\begin{align}
		\left( x_k + \sum_{i=1}^{n-k-1} x_i x_{i+k} \right) x_n - c_k &=0 \quad 1 \leq k \leq n-1\\
		\sum_{l=1}^{n-1} x_l + 1 &= 0
	\end{align}
\end{subequations}
\medskip

Die globalen Optima der jeweiligen Gleichungssystem sind:

\underline{Nichtlineares Gleichungssystem \ref{nse:1}:}
\vspace{-0.2cm}
\begin{align*}
	x^{\ast}_1 = \{-0.6998397867\},\; x^{\ast}_2 = \{0\},\; x^{\ast}_3 = \{2.4936955491\}
\end{align*}

\underline{Zweidimensionale Rosenbrock-Funktion \ref{nse:rosenbrock}:}
\vspace{-0.2cm}
\begin{align*}
	x^{\ast} = \{1, 1\}
\end{align*}

\underline{$n$-dimensionales \textit{Economics Modeling Application}-Problem (für $n=10$ und $c_k = 0$) \ref{nse:highdimension}:}
\vspace{-0.2cm}
\begin{align*}
	x^{\ast} = \{-1.105, 0.461, 0.939, 0.396, -0.181, -0.459, -0.620, -0.293, -0.138, 0.000\}
\end{align*}

In diesem Kapitel wurden die wesentlichen Schritte zur Implementierung der Problemstellung beleuchtet, beginnend mit der Auswahl der Programmiersprache und Bibliothek bis hin zur Beschreibung relevanter Methoden für die Umsetzung des Agenten und der Umgebung. Dabei wird \textit{Python} als Programmiersprache gewählt, da sie eine benutzerfreundliche Integration mit der Reinforcement-Learning-Bibliothek \glqq stable-baselines3\grqq{} bietet, die eine breite Palette von wichtigen RL-Algorithmen bereitstellt. Die Umgebung wird speziell für die Problemstellung nichtlinearer Gleichungssysteme entwickelt, wobei eine angepasste Initialisierung und Berechnung des Residuums eine zentrale Rolle spielen. Ein Ausschnitt zur Ausführung eines Lernschritts wurde ebenfalls präsentiert, um den Prozess der Lösungsfindung innerhalb der Umgebung zu verdeutlichen.

Im weiteren Verlauf wurden drei nichtlineare Gleichungssysteme vorgestellt, die für die Auswertung des Verfahrens herangezogen wurden: ein eindimensionales nichtlineares Gleichungssystem, die zweidimensionale Rosenbrock-Funktion und ein $n$-dimensionales \textit{Economics Modeling Application}-Problem (vgl. \cite{CrinaGrosan2008}). Die globalen Optima der jeweiligen Gleichungssysteme werden ebenfalls präsentiert, um einen Überblick über die erwarteten Lösungen zu geben.

Die Umsetzung dieses Kapitels legt das Fundament für die Experimente und Ergebnisse, in denen die Leistungsfähigkeit des entwickelten RL-Agenten bei der Lösung nichtlinearer Gleichungssysteme bewertet wird. Diese werden im nachfolgenden Kapitel dargestellt.

\section{Ergebnisse}

Für den Ansatz und für die gegebenen Problemstellungen haben sich folgende Ergebnisse ergeben:
\smallskip


\begin{table}[h]
	\centering
	\begin{tabular}{c||c|c|c|c}
		\textbf{NSE} & \textbf{Epochs} & \textbf{Time (s)} & \textbf{Best Action} & \textbf{Best Residuum} \\
		\hline
		\multirow{4}{*}{NSE 1} & $1,000$ & $3.794$ & $-0.69869$ & $0.0065$ \\
		& $5,000$ & $15.402$ & $-0.69986$ & $0.00015$ \\
		& $10,000$ & $39.177$ & $-0.69982$ & $5.79 \times 10^{-5}$ \\
		& $50,000$ & $548.275$ & $-3.08 \times 10^{-5}$ & $6.17 \times 10^{-5}$ \\
		\hline
		\multirow{4}{*}{NSE 2} & $1,000$ & $3.305$ & $[0.993, 0.971]$ & $0.1488$ \\
		& $5,000$ & $9.898$ & $[1.061, 1.145]$ & $0.1997$ \\
		& $10,000$ & $15.447$ & $[0.920, 0.834]$ & $0.1421$ \\
		& $50,000$ & $107.568$ & $[0.891, 0.795]$ & $0.1101$ \\
		\hline
		\multirow{4}{*}{NSE 3} & $1,000$ & $3.052$ & $[0.209, -0.705, ...]$ & $0.5992$ \\
		& $5,000$ & $9.145$ & $[1.217, -0.910, ...]$ & $0.7148$ \\
		& $10,000$ & $16.307$ & $[0.303, 0.301, ...]$ & $0.7161$ \\
		& $50,000$ & $97.955$ & $[0.046, 0.465, ...]$ & $0.5992$ \\
	\end{tabular}
	\caption{Ergebnisse des Agenten für unterschiedliche Problemstellungen}
	\label{tab:Ergebnisse}
\end{table}

\begin{table}[h]
	\centering
	
	\begin{tabular}{c||c|c|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{2.5cm}}
		\textbf{NSE} & \textbf{Steps} & \textbf{Time} (in s) & \textbf{Startpunkt} $\mathbf{x_0}$ & \textbf{Solution} & \textbf{Tolerance}  \\
		\hline
		\multirow{4}{*}{NSE 1 (\ref{nse:1})} & $5$ & $0.005$ & $5.0$ & $2.5777$ & $0.1919$\\
		& $10$ & $0.007$ & $5.0$ & $2.4937$ & $7 \times 10^{-15}$\\
		& $15$ & $0.010$ & $5.0$ & $2.4936$ & $0.0$\\
		& $20$ & $0.013$ & $5.0$ & $2.4936$ & $0.0$\\
		\hline
		\multirow{4}{*}{NSE 2 (\ref{nse:rosenbrock})} & $5$ & $0.004$ & $[-5.0, 5.0]$ & $[1, 1]$ & $0.0$\\
		 & $10$ & $0.006$ & $[-5.0, 5.0]$ & $[1, 1]$ & $0.0$\\
		 & $15$ & $0.008$ & $[-5.0, 5.0]$ & $[1, 1]$ & $0.0$\\
		 & $20$ & $0.010$ & $[-5.0, 5.0]$ & $[1, 1]$ & $0.0$\\
	\end{tabular}
	\caption{Ergebnisse des Newton-Raphson-Verfahrens für NSE 1 \& NSE 2}
	\label{tab:Ergebnisse-Newton}
\end{table}

\begin{figure}[h]
	\centering
	\begin{subfigure}
		\centering
		\includegraphics[width=0.45\textwidth]{Bilder/solutions_nse1.png}
		%\caption{Gefundene Lösung für NSE 1 (\ref{nse:1}) mit einem Schwellenwert $\leq 1e-5$}
		\label{fig:solutions_nse1}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.45\textwidth]{Bilder/solutions_nse2.png}
		%\caption{Gefundene Lösung für NSE 2 (\ref{nse:2}) mit einem Schwellenwert $\leq 1e-3$}
		\label{fig:solutions_nse2}
	\end{subfigure}
	\caption{Gefundene Lösungen für NSE 1 (mit Schwellenwert $\leq 1 \times 10^{-3}$) und NSE 2 (mit Schwellenwert $\leq 1$)}
	\label{fig:solutions_nse1_nse2}
\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}
		\centering
		\includegraphics[width=0.45\textwidth]{Bilder/best_residuen.png}
		%\caption{Gefundene Lösung für NSE 1 (\ref{nse:1}) mit einem Schwellenwert $\leq 1e-5$}
		\label{fig:best_residuen}
	\end{subfigure}
	\hfill
	\begin{subfigure}
		\centering
		\includegraphics[width=0.45\textwidth]{Bilder/threshold_over_time.png}
		%\caption{Gefundene Lösung für NSE 2 (\ref{nse:2}) mit einem Schwellenwert $\leq 1e-3$}
		\label{fig:good_points_thrs}
	\end{subfigure}
	\caption{Entwicklung der besten Residuen über die Zeit \& Die Entwicklung des \glqq gute\grqq{} Punkte Schwellenwertes über die Zeit.}

\end{figure}


In den vorliegenden Ergebnissen fällt besonders auf, dass das vermeintlich einfachste Problem (NSE 1, \ref{nse:1}) die längste Zeit benötigt, um alle 50.000 Epochen abzuschließen - etwa 550 Sekunden. Im Gegensatz dazu liegt die Dauer für die Absolvierung der gleichen Anzahl von Epochen für die Probleme \ref{nse:rosenbrock} und \ref{nse:highdimension} in einer Größenordnung von nur knapp 100 Sekunden. Eine mögliche Erklärung dafür liegt darin, dass das Residuum für das erste Problem (NSE 1, \ref{nse:1}) sehr schnell in einen extrem kleinen Bereich fällt. Dadurch erfordert selbst eine geringfügige Verbesserung des Ergebnisses bereits sehr kleine Anpassungen in den Aktionen des Agenten. Allerdings könnte der Agent aufgrund von Beschränkungen in seiner Aktionsraumgröße Schwierigkeiten haben, Änderungen im Bereich von $1 \times 10^{-5}$ zu erreichen. Dadurch wird möglicherweise keine signifikante Verbesserung erzielt, sodass die Optimierung stagniert und die Zeit zur Absolvierung der 50000 Epochen zunimmt. 

Jedoch wäre dies nur eine anfängliche Idee, die tiefer untersucht werden müsste, da für die Konvergenz des Verfahrens viele weitere Parameter einen Einfluss ausüben.
\medskip

Bei Problem 1 (NSE 1 \ref{nse:1}) ist zudem die Besonderheit, dass das nichtlineare Gleichungssystem mehrere globale Optima besitzt (in diesem Fall genau drei). Die meisten Verfahren, wie zum Beispiel das Newton-Raphson-Verfahren benötigen einen anfänglichen Startpunkt, und nähern sich dann iterativ genau \textbf{einer} Lösung an, sofern sie denn existiert. Beim vorliegenden Verfahren ist die Besonderheit, dass zum Teil mehrere Lösungen gefunden werden können, da der Agent allein lernt, die Belohnung zu maximieren. Da das Residuum für alle Lösungen gleich null ist, ist es möglich, mehrere Lösungen gleichzeitig mit ausreichender Genauigkeit zu identifizieren. Wie in Tabelle \ref{tab:Ergebnisse} dargestellt, konvergierte der Agent mit einer Anzahl von $1000$, $5000$ und $10000$ Epochen zunehmend zur Lösung bei $x^{\ast} \approx -0.6998$. Bei einer weiter erhöhten Epochenanzahl von $50000$ gelang es dem Agenten zusätzlich, sich der Lösung bei $x^{\ast} = 0$ anzunähern.

In den seltensten Fällen jedoch wurde die Lösung bei $x^{\ast} \approx 2.4936$ mit einer Genauigkeit von $\leq 1 \times 10^{-3}$ gefunden. Grund dafür ist die große Sensibilität beim Residuum, denn in diesem Bereich weist die Funktion $f(x) = x^5 - 3x^4+x^3+0.5x^2 - \sin(x)$ eine große Steigung auf, sodass kleine Änderungen im Wert von $x$ hier zu erheblichen Änderungen im Funktionswert $y$ führen. Dies hat zur Folge, dass die Belohnung für kleinere Änderungen in der Aktion stärker schwankt, sodass diese Aktionen im Mittel nicht vielversprechend genug sind. Der Agent konzentriert sich dann eher auf die Aktionen, die im Mittel eine höhere Belohnung liefern, sodass der Agent diese auch weiter verfolgt und in der näheren Umgebung dieser Aktionen mehr erkundet.

\begin{figure}[h]
	\centering
	\includegraphics[width=10cm]{Bilder/reward_action_inverse.png}
	\caption{Belohnung in Abhängigkeit von der Aktion für NSE 1 (\ref{nse:1}).}
	\label{fig:reward-action-nse1-inverse}
\end{figure}
\medskip

Weiterhin ist zu beobachten, dass bei NSE 2 (siehe \ref{nse:rosenbrock}), eine Erhöhung der Epochenanzahl auf $5000$ nicht zu verbesserten Ergebnissen führt. Tatsächlich ist die bei dieser höheren Epochenanzahl gefundene optimale Aktion schlechter als die beste Aktion, die bei einer niedrigeren Zahl von $1000$ Epochen identifiziert wurde.

Die könnte mit der Zufälligkeit des Agenten zusammenhängen, da der Agent zu Beginn zufällig im gegebenen Raum initialisiert wird und auch mit jedem Aufruf der \code{reset} Methode zufällig zurückgesetzt wird.

Zudem wird auch hier die Problematik des nichtlinearen Gleichungssystems deutlich. Die Rosenbrock-Funktion besteht aus einem \glqq Banana\grqq{}-förmigen Tal, indem die Residuen schnell sehr klein werden, sodass es eine Herausforderung darstellt das globale Optimum effizient zu finden. 
\medskip

Es ist nicht überraschend, dass die Genauigkeit der Lösung für NSE 3 (\ref{nse:highdimension}) bei gleicher Anzahl an Epochen geringer ist. Dies liegt daran, dass der Aktionsraum in diesem Fall 10-dimensional ist. Der Agent hat daher Schwierigkeiten, die Richtung zu bestimmen, in der sich das Residuum verbessern könnte. Mit steigender Dimension könnte sich diese Problematik weiter verschärfen, sodass der Agent bzw. der grundlegende Algorithmus für diese Problemstellung optimiert werden muss.
\medskip

In Abbildung \ref{fig:reward-action-nse1-inverse} ist die Belohnung in Abhängigkeit der gewählten Aktion für NSE 1 (\ref{nse:1}) zu sehen, mit einer zusätzlichen Verdeutlichung um die Lage der tatsächlichen globalen Optima. Es ist zu erkennen, dass die Belohnung größer wird, je näher die gefundene Lösung an der tatsächlichen Lösung befindet. Dank dieser Darstellung lässt sich auch erklären, weshalb der Agent in den seltensten Fällen die Lösung bei $2.49$ findet, da das Residuum sehr stark für kleine Änderungen in den Aktionen schwankt und so die Belohnung in diesem Bereich spärlich verteilt ist. 
\medskip

Grundlegend bei RL ist vor allen Dingen die \glqq teure\grqq{} Berechnung. Da der PPO-Algorithmus verwendet wurde, müssen dabei in jedem Schritt zwei neuronale Netze aktualisiert werden (Actor- und Critic-Netzwerk). Dies ist insbesondere im Vergleich zum relativ simplen Newton-Raphson-Verfahren sehr teuer und ein nicht zu vernachlässigender Faktor, der sich auf die Zeit auswirkt, bis zu dem eine Lösung mit einer bestimmten Genauigkeit gefunden wird.
\\

In diesem Kapitel wurden die wichtigsten Ergebnisse vorgestellt, die mit dem Ansatz erzielt wurden und untereinander verglichen. Es wurde auf Besonderheiten eingegangen und versucht mögliche Erklärungen zu finden. Abschließend dazu kann nun ein zusammenfassendes Fazit formuliert werden.


\section{Fazit}

Als Fazit kann formuliert werden, dass das präsentierte Verfahren grundsätzlich in der Lage ist Lösungen von nichtlinearen Gleichungssystemen zu finden. Besser funktioniert das Verfahren für eindimensionale Probleme, bei denen die Lösungen robust sind, also nicht sensibel auf kleine Änderungen der Aktion reagieren. Für solche Probleme ist das Verfahren mit dargestellter Umsetzung in der Lage Lösungen mit einer Genauigkeit von ungefähr $3 \times 10^{-5}$ finden. Vor dem Hintergrund der Zeit und der Berechnungskomplexität allerdings, schließt dieses Verfahren insbesondere im Vergleich zu dem etablierten Newton-Raphson-Verfahren schlecht ab. Dieses Verfahren findet mit weniger Berechnungen und in kürzerer Dauer die Lösung mit einer höheren Genauigkeit. Jedoch sollte dabei auch erwähnt sein, dass dieses Verfahren einen initialen Startpunkt benötigt, von dem das Verfahren sich iterativ der nächsten Lösung annähert. Demzufolge auch nur eine Lösung findet.
\smallskip

Je nach Anforderung des spezifischen Anwendungsfalls kann es auch notwendig sein mehrere Lösungen zu identifizieren, sodass diesbezüglich das vorgestellte Verfahren von Vorteil sein kann. Des Weiteren kann sich dieser Ansatz auch als günstig erweisen, um vielversprechende Startpunkte für iterative Verfahren ausfindig zu machen, sodass die grobe Lage möglicher Lösungen geschätzt werden kann. Iterative Verfahren könnten daran anknüpfen und diese Lösungen nutzen, um Lösungen mit höherer Genauigkeit zu finden. 
\medskip

Bei der Anwendung dieses Verfahrens auf die Rosenbrock-Funktion (NSE 2, \ref{nse:rosenbrock}), wurde deutlich, dass auch dieses Verfahren Schwierigkeiten hat, das globale Optimum effizient zu finden. Zusätzliche Informationen, die spezifisch für das Problem sind, können helfen, das globale Optimum besser und effizienter zu finden. Allerdings könnten solche Informationen für andere Problemstellungen weniger relevant sein, was ihre breite Anwendbarkeit einschränken könnte.
\medskip

Insbesondere bei nichtlinearen Gleichungen höherer Dimension deutet sich an, dass die Genauigkeit der gefundenen Lösungen stark abnimmt. Während für NSE 1 und NSE 2 Lösungen mit einer Genauigkeit von ungefähr $3 \times 10^{-5}$ für NSE 1 und $0.1$ für NSE 2 gefunden werden konnten, befand sich der Fehler der besten gefundene Lösung für NSE 3 (mit einer Dimension von  $10$) in einer Größenordnung von $0.59$ bei gleichbleibender Epochen-Anzahl.

Weiterhin muss geklärt werden, ob sich die Präzision der Lösungen generell verringert, je mehr Dimensionen das Problem aufweist. Zur Beantwortung dieser Frage bedarf es detaillierter Tests und eingehender Untersuchungen, die es erlauben, die Effektivität des Reinforcement-Learning-Agenten bei unterschiedlichen Problemumfängen zu bewerten. Valide Schlüsse lassen sich erst nach einer umfassenden Analyse und sorgfältigen Bewertung ziehen.
\\

Abschließend ist anzumerken, dass neben den bereits diskutierten Parametern eine Vielzahl weiterer Faktoren die Lösungsfindung und die Qualität der Lösung beeinflussen können. Diese zusätzlichen Faktoren können die Effektivität des Verfahrens beeinträchtigen und sollten daher ebenfalls berücksichtigt werden. Dazu gehören:

\begin{itemize}
	\item Die Größe des Aktionsraumes
	\item Die Definition des Zustandes
	\item Die Berechnung und Skalierung des Residuums
	\item Die Gestaltung der Belohnungsfunktion
	\item Der zugrunde liegende RL-Algorithmus mit allen zugehörigen beeinflussbaren Hyperparametern, wie Lernrate, Batch-Size usw.
\end{itemize}

Diese Arbeit bietet somit Einblicke in die Möglichkeiten und Herausforderungen bei der Anwendung von Reinforcement-Learning zur Lösung nichtlinearer Gleichungssysteme und legt den Grundstein für weitere Untersuchungen auf diesem Gebiet. 


\newpage

\clearpage % or \cleardoublepage

\addcontentsline{toc}{section}{Literaturverzeichnis}

\printbibliography

\newpage

\section{Eigenständigkeitserklärung}

Ich habe diese Arbeit selbstständig und ohne fremde Hilfe Dritter angefertigt. Bei der Übernahme fremder Gedanken habe ich dies als Zitat kenntlich gemacht. 

Bielefeld, 06.04.2024

Nicolas Schneider

\end{onehalfspace}

\end{document}








